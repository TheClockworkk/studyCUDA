{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheClockworkk/studyCUDA/blob/main/course_work_PGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdJdfE0Jv1aN",
        "outputId": "96215956-addf-4002-a040-413c1ddc0a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.8/dist-packages (3.1.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install mpi4py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6izRUfJeJD6",
        "outputId": "c40a3819-2b75-4a4f-9ff4-54efe2be542d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQoaT37jv6WF",
        "outputId": "97f86601-b17e-4f48-fdf6-e0a772780b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_set.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_set.py\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "train_data = datasets.MNIST(\n",
        "      root = 'data',\n",
        "      train = True,                         \n",
        "      transform = ToTensor(), \n",
        "      download = True,            \n",
        "  )\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU_uNJEDv6dS",
        "outputId": "a583d65f-9d53-4a52-aad5-2398eace47cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_loader.py\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from data_set import train_data,test_data\n",
        "def DataLoader(batchSize, numWorkers,shuffle = False,):\n",
        "  loaders = {\n",
        "  'train' : torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=batchSize, \n",
        "                                          shuffle=shuffle, \n",
        "                                          num_workers=numWorkers),\n",
        "  \n",
        "  'validate'  : torch.utils.data.DataLoader(test_data, \n",
        "                                          batch_size=batchSize, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=numWorkers)\n",
        "  }\n",
        "  return loaders     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP10eRHmv6fQ",
        "outputId": "e4813428-2ef5-4393-baa1-e3a378730b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q98DY13Dv6i8",
        "outputId": "02ea2c89-4e6a-45c1-84bd-7054adf1c8d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from mpi4py import MPI\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from data_loader import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from model import CNN\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "def train_model(num_epochs,\n",
        "                criterion,\n",
        "                test_dataloader,\n",
        "                rank,\n",
        "                batch_size,\n",
        "                optimizer,\n",
        "                model,\n",
        "                train_dataloader,\n",
        "                val_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model_best = 0\n",
        "    for epoch in range(num_epochs):\n",
        "      running_loss = 0\n",
        "      accuracy = 0\n",
        "\n",
        "      dataset_sizes_train = len(train_dataloader)\n",
        "      model.train()\n",
        "      if rank == 0:\n",
        "        i = 0\n",
        "        for images, labels in tqdm(train_dataloader):\n",
        "          comm.send((images, labels),dest = i%(p-1)+1)\n",
        "          i+=1\n",
        "      if rank != 0:\n",
        "        for i in range(len(train_dataloader)):    \n",
        "          if i % (p - 1) + 1 == rank:\n",
        "            (images,lables) = comm.recv(source=0)\n",
        "            images = images.to(device)\n",
        "            lables = lables.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output,lables)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * images.size(0)\n",
        "        running_loss = running_loss / dataset_sizes_train\n",
        "        print(\"Epoch of train:\", epoch + 1,\"Loss: [\", running_loss, \"]\", \"rank: \", rank)\n",
        "            \n",
        "      MPI.Comm.Barrier(MPI.COMM_WORLD)\n",
        "      \n",
        "      accuracy = 0\n",
        "      validate_loss = 0.0\n",
        "      dataset_sizes_val = len(val_dataloader)\n",
        "      if rank != 0:\n",
        "        model.eval()\n",
        "      if rank == 0:\n",
        "        i = 0\n",
        "        for images, labels in tqdm(val_dataloader):\n",
        "          comm.send((images,labels), dest=i % (p - 1) + 1)\n",
        "          i+=1\n",
        "      if rank != 0:\n",
        "        for i in range(len(val_dataloader)):    \n",
        "          if i % (p - 1) + 1 == rank:\n",
        "            (images,lables) = comm.recv(source=0)\n",
        "            images = images.to(device)\n",
        "            lables = lables.to(device)\n",
        "            with torch.no_grad():\n",
        "              output = model(images)\n",
        "              loss = criterion(output,lables)\n",
        "              validate_loss += loss.item() * images.size(0)\n",
        "              pred_y = torch.max(output, 1)[1].data.squeeze()\n",
        "        validate_loss = validate_loss / dataset_sizes_val\n",
        "        print(\"Epoch of validation:\", epoch + 1,\"Loss: \",validate_loss, rank)\n",
        "      MPI.Comm.Barrier(MPI.COMM_WORLD) \n",
        "      if rank != 0: \n",
        "        if epoch == 0:\n",
        "          model_best = validate_loss\n",
        "        if validate_loss <= model_best:\n",
        "          model_best = validate_loss\n",
        "          torch.save(model.state_dict(), f\"./weights/model_{rank}.pth\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  comm = MPI.COMM_WORLD\n",
        "  my_rank = comm.Get_rank()\n",
        "  p = comm.Get_size()\n",
        "  num_epochs = 5\n",
        "  batch_size = 20\n",
        "  num_workers = 4\n",
        "  train_dataloader = DataLoader(batch_size,num_workers)['train']\n",
        "  validate_dataloader = DataLoader(batch_size,num_workers)['validate']\n",
        "  model = CNN()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = train_model(num_epochs,criterion,validate_dataloader,my_rank,\n",
        "                           batch_size,optimizer,model,train_dataloader,validate_dataloader)\n",
        "  MPI.Finalize               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIFOs50rv6kz",
        "outputId": "eac36434-da95-44ba-9ea3-a889a7250480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|█████████▉| 2999/3000 [01:54<00:00, 24.70it/s]Epoch of train: 1 Loss: [ 1.2310641736692438 ] rank:  5\n",
            "Epoch of train: 1 Loss: [ 1.235979883239294 ] rank:  6\n",
            "Epoch of train: 1 Loss: [ 1.166913262422507 ] rank:  7\n",
            "100%|██████████| 3000/3000 [01:54<00:00, 26.16it/s]Epoch of train: 1 Loss: [ 1.2304708050222446 ] rank:  1\n",
            "Epoch of train: 1 Loss: [ 1.213179848107199 ] rank:  2\n",
            "Epoch of train: 1 Loss: [ 1.1995475179019073 ] rank:  3\n",
            "Epoch of train: 1 Loss: [ 1.272799424980767 ] rank:  4\n",
            "\n",
            " 99%|█████████▉| 494/500 [00:16<00:00, 25.64it/s]Epoch of validation: 1 Loss:  0.392328863888979 5\n",
            "Epoch of validation: 1 Loss:  0.476058896407485 4\n",
            " 99%|█████████▉| 497/500 [00:16<00:00, 22.70it/s]Epoch of validation: 1 Loss:  0.43295906245708465 6\n",
            "Epoch of validation: 1 Loss:  0.35578841557726265 7\n",
            "100%|██████████| 500/500 [00:17<00:00, 23.92it/s]Epoch of validation: 1 Loss:  0.4040679756179452 1\n",
            "Epoch of validation: 1 Loss:  0.41868364810943604 2\n",
            "100%|██████████| 500/500 [00:17<00:00, 29.14it/s]Epoch of validation: 1 Loss:  0.33858619462698697 3\n",
            "\n",
            "100%|█████████▉| 2996/3000 [01:53<00:00, 25.62it/s]Epoch of train: 2 Loss: [ 0.37961047697501876 ] rank:  6\n",
            "100%|██████████| 3000/3000 [01:53<00:00, 26.96it/s]Epoch of train: 2 Loss: [ 0.3529171429372703 ] rank:  5\n",
            "100%|██████████| 3000/3000 [01:53<00:00, 26.38it/s]Epoch of train: 2 Loss: [ 0.3607216626467804 ] rank:  1\n",
            "Epoch of train: 2 Loss: [ 0.33896384967491033 ] rank:  7\n",
            "Epoch of train: 2 Loss: [ 0.3628519396840905 ] rank:  2\n",
            "Epoch of train: 2 Loss: [ 0.3331247164402157 ] rank:  3\n",
            "Epoch of train: 2 Loss: [ 0.3569558973641445 ] rank:  4\n",
            "\n",
            " 99%|█████████▉| 494/500 [00:14<00:00, 35.85it/s]Epoch of validation: 2 Loss:  0.341534913405776 4\n",
            "Epoch of validation: 2 Loss:  0.2966058489168063 5\n",
            "Epoch of validation: 2 Loss:  0.3699286416219547 6\n",
            "Epoch of validation: 2 Loss:  0.2561984748020768 2\n",
            "100%|█████████▉| 499/500 [00:14<00:00, 37.68it/s]Epoch of validation: 2 Loss:  0.2411364760249853 1\n",
            "Epoch of validation: 2 Loss:  0.2024525234568864 7\n",
            "100%|██████████| 500/500 [00:14<00:00, 33.70it/s]Epoch of validation: 2 Loss:  0.26513961151242255 3\n",
            "\n",
            "100%|█████████▉| 2995/3000 [01:42<00:00, 35.93it/s]Epoch of train: 3 Loss: [ 0.22234286542826642 ] rank:  5\n",
            "100%|█████████▉| 2999/3000 [01:42<00:00, 32.31it/s]Epoch of train: 3 Loss: [ 0.21889210241885546 ] rank:  7\n",
            "100%|██████████| 3000/3000 [01:42<00:00, 29.21it/s]Epoch of train: 3 Loss: [ 0.24666290879870453 ] rank:  6\n",
            "Epoch of train: 3 Loss: [ 0.22283396104816347 ] rank:  1\n",
            "Epoch of train: 3 Loss: [ 0.23202366466013094 ] rank:  2\n",
            "Epoch of train: 3 Loss: [ 0.2381812206748873 ] rank:  4\n",
            "Epoch of train: 3 Loss: [ 0.2191033287202784 ] rank:  3\n",
            "\n",
            " 99%|█████████▉| 495/500 [00:14<00:00, 35.49it/s]Epoch of validation: 3 Loss:  0.24703902912326156 4\n",
            "Epoch of validation: 3 Loss:  0.23121431059204042 5\n",
            "Epoch of validation: 3 Loss:  0.27022984275594353 6\n",
            "100%|█████████▉| 499/500 [00:14<00:00, 35.89it/s]Epoch of validation: 3 Loss:  0.17669766390928998 7\n",
            "Epoch of validation: 3 Loss:  0.2515711170807481 1\n",
            "100%|██████████| 500/500 [00:14<00:00, 34.04it/s]Epoch of validation: 3 Loss:  0.25803566111717374 2\n",
            "Epoch of validation: 3 Loss:  0.22002421418670565 3\n",
            "\n",
            "100%|█████████▉| 2999/3000 [01:46<00:00, 28.28it/s]Epoch of train: 4 Loss: [ 0.1562451883708127 ] rank:  5\n",
            "Epoch of train: 4 Loss: [ 0.1724367854705391 ] rank:  6\n",
            "100%|██████████| 3000/3000 [01:46<00:00, 28.06it/s]Epoch of train: 4 Loss: [ 0.14749658780288882 ] rank:  7\n",
            "Epoch of train: 4 Loss: [ 0.1549920614766112 ] rank:  1\n",
            "Epoch of train: 4 Loss: [ 0.16014333082363008 ] rank:  2\n",
            "Epoch of train: 4 Loss: [ 0.14746835518764176 ] rank:  3\n",
            "Epoch of train: 4 Loss: [ 0.17008253957921018 ] rank:  4\n",
            "\n",
            " 98%|█████████▊| 489/500 [00:18<00:00, 38.94it/s]Epoch of validation: 4 Loss:  0.1779131195845548 4\n",
            "100%|█████████▉| 498/500 [00:18<00:00, 50.19it/s]Epoch of validation: 4 Loss:  0.18753005950013177 5\n",
            "Epoch of validation: 4 Loss:  0.20386513931676745 6\n",
            "Epoch of validation: 4 Loss:  0.28101367796072735 7\n",
            "Epoch of validation: 4 Loss:  0.19054589356994256 1\n",
            "100%|██████████| 500/500 [00:18<00:00, 27.27it/s]Epoch of validation: 4 Loss:  0.21393670287798158 2\n",
            "Epoch of validation: 4 Loss:  0.2077736496925354 3\n",
            "\n",
            "100%|█████████▉| 2997/3000 [01:41<00:00, 29.41it/s]Epoch of train: 5 Loss: [ 0.10144152919315577 ] rank:  5\n",
            "100%|██████████| 3000/3000 [01:41<00:00, 29.49it/s]Epoch of train: 5 Loss: [ 0.12833173164612768 ] rank:  6\n",
            "Epoch of train: 5 Loss: [ 0.10939664682072665 ] rank:  7\n",
            "Epoch of train: 5 Loss: [ 0.11534379753245351 ] rank:  2\n",
            "Epoch of train: 5 Loss: [ 0.11822669582033996 ] rank:  1\n",
            "Epoch of train: 5 Loss: [ 0.09753031156607904 ] rank:  3\n",
            "Epoch of train: 5 Loss: [ 0.12998961230026906 ] rank:  4\n",
            "\n",
            " 98%|█████████▊| 492/500 [00:16<00:00, 36.68it/s]Epoch of validation: 5 Loss:  0.21572024733410217 4\n",
            "100%|█████████▉| 499/500 [00:16<00:00, 43.93it/s]Epoch of validation: 5 Loss:  0.21898402984952553 5\n",
            "Epoch of validation: 5 Loss:  0.16635575006715952 6\n",
            "Epoch of validation: 5 Loss:  0.20808422997244633 7\n",
            "100%|██████████| 500/500 [00:16<00:00, 30.70it/s]Epoch of validation: 5 Loss:  0.15806059299589834 1\n",
            "Epoch of validation: 5 Loss:  0.2679383232514374 2\n",
            "Epoch of validation: 5 Loss:  0.26666897351969965 3\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "! mpirun --allow-run-as-root -np 8 --oversubscribe python main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U46E3pjpv6mu",
        "outputId": "2b7aa2dc-3c40-464b-b6e5-781acf69a774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.py\n",
        "from mpi4py import MPI\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from data_loader import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from model import CNN\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "def test(model, criterion, dataloader_test, dataset_sizes_test):\n",
        "    score = 0\n",
        "    runing_loss = 0.0\n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        if rank != 0:\n",
        "            print(\"Start proccess number \",rank)\n",
        "            for image, label in tqdm(dataloader_test): \n",
        "                output = model(image)\n",
        "                comm.send(output, dest=0, tag=0) \n",
        "                if rank == 1:\n",
        "                    comm.send(label, dest=0, tag=1)\n",
        "                _, preds = torch.max(output, 1)\n",
        "                loss = criterion(output, label)\n",
        "                runing_loss += loss.item() * image.size(0)\n",
        "                score += torch.sum(preds == label.data)\n",
        "            epoch_acc = score.double() / dataset_sizes_test\n",
        "            runing_loss = runing_loss / dataset_sizes_test\n",
        "            print(\"Test process \", rank, \": score: [\", epoch_acc.item(), \"], loss: [\", runing_loss, \"]\")\n",
        "        \n",
        "        MPI.Comm.Barrier(MPI.COMM_WORLD)\n",
        "        result = 0\n",
        "        if rank == 0:\n",
        "            print(\"Start proccess number \",rank)\n",
        "            for i in tqdm(range(len(dataloader_test))):\n",
        "                label = comm.recv(source=1, tag=1) \n",
        "                for procid in range(1, p):\n",
        "                    output = comm.recv(source=procid, tag=0)\n",
        "                    if procid == 1:\n",
        "                        result_all_models = output\n",
        "                    else:\n",
        "                        result_all_models += output\n",
        "                _, preds = torch.max(result_all_models, 1)\n",
        "                result += torch.sum(preds == label.data)\n",
        "            result = result.double() / dataset_sizes_test\n",
        "            print(\"Test process result\", rank, result.item())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  comm = MPI.COMM_WORLD\n",
        "  rank = comm.Get_rank()\n",
        "  p = comm.Get_size()\n",
        "  num_epochs = 5\n",
        "  batch_size = 20\n",
        "  num_workers = 4\n",
        "  train_dataloader = DataLoader(batch_size,num_workers)['train']\n",
        "  validate_dataloader = DataLoader(batch_size,num_workers)['validate']\n",
        "  model = CNN()\n",
        "  if(rank != 0):\n",
        "     model.load_state_dict(torch.load(f'/content/weights/model_{rank}.pth'))\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = test(model,criterion,validate_dataloader,len(validate_dataloader) * batch_size)\n",
        "  MPI.Finalize    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k6GzG3gwv6pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4063e4fd-7c15-4df0-a1d6-960ebbab4905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start proccess number  1\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\r  0%|          | 0/500 [00:00<?, ?it/s]Start proccess number  5\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\r  0%|          | 0/500 [00:00<?, ?it/s]Start proccess number  7\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]Start proccess number  2\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]Start proccess number  3\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  0%|          | 1/500 [00:01<09:57,  1.20s/it]Start proccess number  6\n",
            "  1%|          | 4/500 [00:01<02:59,  2.77it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  3%|▎         | 15/500 [00:03<01:18,  6.16it/s]Start proccess number  4\n",
            "  3%|▎         | 16/500 [00:03<01:18,  6.13it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 500/500 [01:02<00:00,  8.06it/s]Test process  5 : score: [ 0.9756 ], loss: [ 0.07809840702125803 ]\n",
            "100%|██████████| 500/500 [01:03<00:00,  7.84it/s]Test process  7 : score: [ 0.9725 ], loss: [ 0.08085012069100049 ]\n",
            "100%|██████████| 500/500 [01:03<00:00,  7.85it/s]Test process  2 : score: [ 0.9781 ], loss: [ 0.06751228368951706 ]\n",
            "100%|██████████| 500/500 [01:03<00:00,  7.85it/s]Test process  3 : score: [ 0.9755 ], loss: [ 0.07464875521601061 ]\n",
            "100%|██████████| 500/500 [01:04<00:00,  7.79it/s]Test process  6 : score: [ 0.9771 ], loss: [ 0.07001917942025467 ]\n",
            "100%|██████████| 500/500 [01:06<00:00,  7.56it/s]Test process  1 : score: [ 0.9798 ], loss: [ 0.06407751424903109 ]\n",
            "100%|██████████| 500/500 [01:03<00:00,  7.84it/s]Test process  4 : score: [ 0.9773 ], loss: [ 0.07222219718748238 ]\n",
            "\n",
            "\n",
            "Start proccess number  0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 500/500 [00:06<00:00, 79.66it/s] Test process result 0 0.2874\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! mpirun --allow-run-as-root -np 8 --oversubscribe python test.py\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlEXF8BuxLIAB8pxpCPozF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}